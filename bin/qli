#!/usr/bin/env python

import subprocess
import tempfile
import urllib
import sys
from BeautifulSoup import BeautifulSoup
import urlparse

""" given a url as the only command line argument
    * download the url
        * if the mime type indicates it's an image
            * save to a temp file
            * open qith quicklook (qlmanage -p <img>)
            * clean up tempfile
            * exit
        * if not, scan the links for any that end in image types
"""

# finding the largest image is a neat idea:
# https://github.com/superfeedr/image-extrator/blob/master/scraper.py


def open_url(url, depth=0):

    try:
        uopen = urllib.urlopen(url)
        stream = uopen.read()
    except:
        # if we can't load it for whatever reason, try to back out a level
        return
    urltype = uopen.info().gettype()
    if urltype.startswith('image'):
        image = tempfile.NamedTemporaryFile()
        image.write(stream)
        subprocess.call(['qlmanage', '-p', image.name])
        image.close()
        sys.exit(1)
    elif depth > 1:
        return
    elif urltype.startswith('text'):
        soup = BeautifulSoup(stream)
        images = soup.findAll('img', src=True)
        for i in images:
            image_url = urlparse.urljoin(url, i['src'])
            open_url(image_url, depth + 1)
    else:
        return


def main():
    open_url(sys.argv[1])


if __name__ == '__main__':
    main()
